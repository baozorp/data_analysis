{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPrZSAIIwN8ujsOI7E23bZ8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"w0DcDYaLJP21"},"outputs":[],"source":["!pip install -U ultralytics wandb"]},{"cell_type":"code","source":["import random\n","import shutil\n","import torch\n","import yaml\n","import os\n","\n","import numpy as np\n","\n","from pathlib import Path\n","\n","from ultralytics import YOLO\n","\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"AsJQRjVlJRPq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"x8tyu1O3JTmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.copy2('/content/drive/MyDrive/ITaS/data/best.pt', 'best.pt')"],"metadata":{"id":"WB5Bj2ihJVYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import wandb\n","from wandb.integration.ultralytics import add_wandb_callback\n","\n","wandb.login(key='1de7addec37341330a8aef9bfc7be382cc9c2824')"],"metadata":{"id":"I_DVWOAgJW5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_yaml(output_yaml_path, train_image_dir, val_image_dir, test_image_dir=' ', nc=10):\n","\n","    names = ['lungs', 'trachea', 'bronchitis', 'pneumonia/bronchopneumonia', 'pulmonary edema',\n","             'hydrothorax', 'pneumothorax', 'tracheal collapse', 'neoplasm', 'atelectasis']\n","\n","    yaml_data = {\n","        'names': names,\n","        'nc': nc,\n","        'train': train_image_dir,\n","        'val': val_image_dir,\n","        'test': test_image_dir\n","    }\n","\n","    with open(output_yaml_path, 'w') as j:\n","        yaml.dump(yaml_data, j, default_flow_style=False)"],"metadata":{"id":"qdMY5kh1JYxP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculation_entropy_sampling_dataset(\n","    confidence_threshold: float = 0.01\n","):\n","    path_to_dataset = '/content/drive/MyDrive/ITaS/data/full_train'\n","\n","    # train_labels_paths не создаем, так как пройдя через алгоритм Least Confidence\n","    # мы получим урезанный список картинок, которму train_labels_paths не будет соответствовать.\n","    # Легче потом просто использовать str.replace(), так как наименование файлов одинаковое.\n","    train_images_paths = []\n","\n","    val_images_paths = []\n","    val_labels_paths = []\n","\n","    # Проходимся по директории полного датасета\n","    for root, _, files in os.walk(path_to_dataset):\n","        for file in files:\n","            # train часть\n","            if r'\\train\\images' in root:\n","                train_images_paths.append(os.path.join(root, file))\n","            # val часть\n","            elif r'\\val\\images' in root:\n","                val_images_paths.append(os.path.join(root, file))\n","            elif r'\\val\\labels' in root:\n","                val_labels_paths.append(os.path.join(root, file))\n","\n","    # Алгоритм Least Confidence\n","    model = YOLO('best.pt')\n","\n","    selected_train_images_samples = []\n","    for i, image_path in enumerate(train_images_paths):\n","        print(f'{i}/{len(train_images_paths)} ->> {image_path}')\n","        results = model.predict(image_path, conf=confidence_threshold)\n","        result = results[0]\n","\n","        classes_probabilities = result.boxes.conf\n","\n","        # Вычисляем энтропию для текущего изображения\n","        entropy = -np.sum(classes_probabilities.cpu().numpy() * np.log(classes_probabilities.cpu().numpy() + 1e-10))\n","\n","        selected_train_images_samples.append({\n","            'image_path': image_path,\n","            'entropy': entropy\n","        })\n","\n","    # Сортируем по приоритету\n","    selected_train_images_samples.sort(key=lambda x: x['entropy'])\n","\n","    # Вычленяем только пути у сортированного словаря\n","    selected_train_images_samples = [image['image_path'] for image in selected_train_images_samples]\n","    selected_train_labels_samples = [path.replace('images', 'labels').replace('.jpg', '.txt') for path in selected_train_images_samples]\n","\n","    return (selected_train_images_samples, selected_train_labels_samples, val_images_paths, val_labels_paths)"],"metadata":{"id":"--028szHJbWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entropy_sampling_full_dataset = calculation_entropy_sampling_dataset()\n","print(len(entropy_sampling_full_dataset[0]))"],"metadata":{"id":"cctsikq2J8z0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_entropy_sampling_dataset(\n","    train_images_path: list,\n","    train_labels_path: list,\n","    val_images_path: list,\n","    val_labels_path: list,\n","    experiment_name: str,\n","    percentage_from_top: int = None,\n","    alg_name: str = 'entropy_sampling'\n","):\n","    train_images_path = train_images_path[:int(len(train_images_path) * (percentage_from_top / 100))]\n","    train_labels_path = train_labels_path[:int(len(train_labels_path) * (percentage_from_top / 100))]\n","\n","    train_images_dir = os.path.join('data', alg_name, experiment_name, 'train', 'images')\n","    val_images_dir = os.path.join('data', alg_name, experiment_name, 'val', 'images')\n","    train_labels_dir = os.path.join('data', alg_name, experiment_name, 'train', 'labels')\n","    val_labels_dir = os.path.join('data', alg_name, experiment_name, 'val', 'labels')\n","\n","    os.makedirs(train_images_dir, exist_ok=True)\n","    os.makedirs(val_images_dir, exist_ok=True)\n","    os.makedirs(train_labels_dir, exist_ok=True)\n","    os.makedirs(val_labels_dir, exist_ok=True)\n","\n","    for train_image_path, train_label_path in zip(train_images_path, train_labels_path):\n","        shutil.copy2(train_image_path,\n","                      os.path.join(\n","                          'data', alg_name, experiment_name, 'train', 'images',\n","                          train_image_path[train_image_path.find('images') + 7:]))\n","\n","        shutil.copy2(train_label_path,\n","                      os.path.join(\n","                          'data', alg_name, experiment_name, 'train', 'labels',\n","                          train_label_path[train_label_path.find('labels') + 7:]))\n","\n","    for val_image_path, val_label_path in zip(val_images_path, val_labels_path):\n","\n","        shutil.copy2(val_image_path,\n","                      os.path.join(\n","                          'data', alg_name, experiment_name, 'val', 'images',\n","                          val_image_path[val_image_path.find('images') + 7:]))\n","\n","        shutil.copy2(val_label_path,\n","                      os.path.join(\n","                          'data', alg_name, experiment_name, 'val', 'labels',\n","                          val_label_path[val_label_path.find('labels') + 7:]))\n","\n","    yaml_path = os.path.join('data', alg_name, experiment_name, 'data.yaml')\n","    train_path = os.path.join('train', 'images')\n","    val_path = os.path.join('val', 'images')\n","    create_yaml(yaml_path, train_path, val_path)"],"metadata":{"id":"OFhSyEVNJ_hV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_entropy_sampling_dataset(\n","    train_images_path=entropy_sampling_full_dataset[0],     # train_images\n","    train_labels_path=entropy_sampling_full_dataset[1],     # train_labels\n","    val_images_path=entropy_sampling_full_dataset[2],       # val_images\n","    val_labels_path=entropy_sampling_full_dataset[3],       # val_samples\n","    percentage_from_top=1,\n","    experiment_name='1_train'\n",")"],"metadata":{"id":"2mi2OICEKEKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_entropy_sampling_dataset(\n","    train_images_path=entropy_sampling_full_dataset[0],     # train_images\n","    train_labels_path=entropy_sampling_full_dataset[1],     # train_labels\n","    val_images_path=entropy_sampling_full_dataset[2],       # val_images\n","    val_labels_path=entropy_sampling_full_dataset[3],       # val_samples\n","    percentage_from_top=10,\n","    experiment_name='10_train'\n",")"],"metadata":{"id":"iBUyRjC3KG80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_entropy_sampling_dataset(\n","    train_images_path=entropy_sampling_full_dataset[0],     # train_images\n","    train_labels_path=entropy_sampling_full_dataset[1],     # train_labels\n","    val_images_path=entropy_sampling_full_dataset[2],       # val_images\n","    val_labels_path=entropy_sampling_full_dataset[3],       # val_samples\n","    percentage_from_top=20,\n","    experiment_name='20_train'\n",")"],"metadata":{"id":"--t3ZFfZKHh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_yolo_model(main_experiment: str, name: str, seed: int):\n","    wandb.init(project='ITaS', job_type='training')\n","\n","    model = YOLO(\"yolov8m-seg.pt\")\n","\n","    results = model.train(\n","        data = os.path.join('/content/drive/MyDrive/ITaS' ,'data', main_experiment, name, 'data.yaml'),\n","        project = 'ITaS',\n","        name = name,\n","        epochs = 25,\n","        patience = 0,\n","        batch = 5,\n","        imgsz = 640,\n","        seed=seed\n","    )\n","    wandb.finish()"],"metadata":{"id":"_zH2_7wUKR7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(5):\n","    train_yolo_model(main_experiment='entropy_sampling', name=f'1_train', seed=(i + 1) * 10)"],"metadata":{"id":"qdEe_TIAKL-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(5):\n","    train_yolo_model(main_experiment='entropy_sampling', name=f'10_train', seed=(i + 1) * 11)"],"metadata":{"id":"2dkxyEDrKOZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(5):\n","    train_yolo_model(main_experiment='entropy_sampling', name=f'20_train', seed=(i + 1) * 12)"],"metadata":{"id":"3p6xwZ-XKO1c"},"execution_count":null,"outputs":[]}]}